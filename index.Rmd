---
title       : Sparse inverse covariance estimation with the graphical lasso
subtitle    : Friedman, Jerome, Hastie, Trevor, Tibshirani, Robert
author      : Keith Hughitt
job         : 
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}

--- .segue .dark

## Graphical Models

---

## Graphical Models

Graphical models provide a way to represent the conditional dependencies 
between a number of random variables. They provide a visual way of representing 
the joint distribution of the entire set of RVs.

<span class='red'>Components:</span>
- Vertices: random variables
- Edges: condition dependencies between RVs

<span class='red'>Types:</span>
- Directed: `Bayesian networks` (causal relationships)
- Undirected: `Markov random fields` / `Markov networks`

See [The Elements of Statistical Learning (ESM)](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
chapter 17 for an overview of undirected graphical models.

---

## Graphical Models

Constructing graphical models from data:

* <span class='blue2'>Model selection</span>: choosing the structure of the graph.
* <span class='blue2'>Learning</span>: Estimating edge weights from data.

```{r pgm_example, echo=FALSE}
library(igraph)
x = matrix(0, 5, 5)
x[1, 2] = x[1,3] = x[1,4] = x[2, 4] = x[2, 5] = x[1,5] = x[3,5] = 1
plot(graph.adjacency(x, mode='undirected'))
```
---

## Undirected Graphical Models

### Gaussian Graphical Models
- Assume that the observations have a multivariate Gaussian distribution with
mean $\mu$ and covariance matrix $\Sigma$.

- The <span class='blue'>inverse covariance matrix</span> 
  $\Theta = \Sigma^{-1}$ (aka concentration matrix or precision matrix) 
  contains information about the <span class='blue'>partial covariances</span> 
  between each pair of nodes conditioned on all other variables (ESL.)

- If the $ij$th component of $\Theta$ is zero, then variables $i$ and $j$ are
  conditionally independent, given the other variables.

### Covariance graphs

- In covariance graphs or relevance networks, edges are present when the
<span class='blue'>covariance</span> is non-zero.

---

## Sparse Graphical Models

>- In some cases, you expect that the underlying data should be sparse.
>- Want to only keep significant edges.
>- An $L_1$ penalty on the estimation of $\Sigma^{-1}$ can be used to induce 
   sparseness.

---.segue .dark

## Using the Lasso for Sparse Graphical Models

---

## Meinshausen and BÃ¼hlmann (2006)

- Which components of $\Sigma^{-1}$ are non-zero?
- Fit a lasso regression for each variable using all other variables as
  predictors.
- Considered nonzero if $\Sigma^{-1}_{ij} \neq 0$ AND/OR 
  $\Sigma^{-1}_{ji} \neq 0$
- Shown asymptotically to find nonzero components

---

## Exact solutions

Other authors have suggested exact solutions:

- Yuan & Lin (2007)
- Banerjee et al. (2007)
- Dahl et al. (2007)

[Interior point optimization](http://en.wikipedia.org/wiki/Interior_point_method)
is used to determine an exact maximiation.

---

## Graphical Lasso

- Exact solution based on coordinate descent approach in Banerjee et al. (2007)

---

## System info

```{r sysinfo}
sessionInfo()
```


