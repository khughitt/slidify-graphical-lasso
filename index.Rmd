---
title       : Sparse inverse covariance estimation with the graphical lasso
subtitle    : Friedman, Jerome, Hastie, Trevor, Tibshirani, Robert
author      : Keith Hughitt
job         : 
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
--- .segue .dark

<!-- Custom Styles -->
<style type='text/css'>
    slides > slide {
        height: 800px;
        margin-top: -400px;
    }
    img {
        max-height: 560px;
        max-width: 964px;
    }
    slide a {border-bottom: none;}
    .references li { font-size: 18px; }
</style>

## Graphical Models

---

## Graphical Models

Graphical models provide a way to represent the conditional dependencies 
between a number of random variables. They provide a visual way of representing 
the joint distribution of the entire set of RVs.

<span class='red'>Components:</span>
- Vertices: random variables
- Edges: condition dependencies between RVs

<span class='red'>Types:</span>
- Directed: `Bayesian networks` (causal relationships)
- Undirected: `Markov random fields` / `Markov networks`

See [The Elements of Statistical Learning (ESM)](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
chapter 17 for an overview of undirected graphical models.

---

## Graphical Models

Constructing graphical models from data:

* <span class='blue2'>Model selection</span>: choosing the structure of the graph.
* <span class='blue2'>Learning</span>: Estimating edge weights from data.

```{r pgm_example, echo=FALSE}
library(igraph)
x = matrix(0, 5, 5)
x[1, 2] = x[1,3] = x[1,4] = x[2, 4] = x[2, 5] = x[1,5] = x[3,5] = 1
plot(graph.adjacency(x, mode='undirected'))
```
---

## Undirected Graphical Models

### Gaussian Graphical Models
- Assume that the observations have a multivariate Gaussian distribution with
mean $\mu$ and covariance matrix $\Sigma$.

- The <span class='blue'>inverse covariance matrix</span> 
  $\Theta = \Sigma^{-1}$ (aka concentration matrix or precision matrix) 
  contains information about the <span class='blue'>partial covariances</span> 
  between each pair of nodes conditioned on all other variables (ESL.)

- If the $ij$th component of $\Theta$ is zero, then variables $i$ and $j$ are
  conditionally independent, given the other variables.

### Covariance graphs

- In covariance graphs or relevance networks, edges are present when the
<span class='blue'>covariance</span> is non-zero.

---

## Sparse Graphical Models

>- In some cases, you expect that the underlying data should be sparse.
>- Want to only keep significant edges.
>- An $L_1$ penalty on the estimation of $\Sigma^{-1}$ can be used to induce 
   sparseness.

---.segue .dark

## Using the Lasso for Sparse Graphical Models

---

## Meinshausen and BÃ¼hlmann (2006)

>- Which components of $\Sigma^{-1}$ are non-zero?
>- Fit a lasso regression for each variable using all other variables as
   predictors.
>- Considered nonzero if $\Sigma^{-1}_{ij} \neq 0$ AND/OR 
   $\Sigma^{-1}_{ji} \neq 0$
>- Shown asymptotically to find nonzero components

---

## Exact solutions

Other authors have suggested exact solutions:

- Yuan & Lin (2007)
- Banerjee et al. (2007)
- Dahl et al. (2007)

[Interior point optimization](http://en.wikipedia.org/wiki/Interior_point_method)
is used to determine an exact maximiation.

---

## Graphical Lasso

- Exact solution based on coordinate descent approach in Banerjee et al. (2007)

<span class='red' style='font-weight: 700;'>Approach</span>

>- $N$ observations, $x_i, i=1,\ldots,N$ with dimension $p$, mean $\mu$ and 
   covariance $\Sigma$
>- let $\Theta = \Sigma^{-1}$ and let $S$ be the <span class='blue'>empirical
   covariance matrix</span>:
    $$S = \frac{1}{N} \sum_{i=1}^N (x_i - \overline{x})(x_i - \overline{x})^T$$
>- <span class='blue'>Goal:</span> Maximize the log-likelihood
    $$\text{log det} \Theta - \text{tr}(S\Theta) - \rho\Vert \Theta \Vert_1$$
   over non-negative definite matrices $\Theta$.
>- Above expression is the "Gaussian log-likelihood of the data, partially
   maximized with respect to the mean parameter $\mu$.

--- .segue .dark

## An interesting methods discussion...

---

## Algorithm (Friedman et al. 2007):

1. Start with $W = S + \rho I$. The diagonal of $W$ remains unchanged in what
follows.
2. For each $j = 1,2,\ldots p,1,2,\ldots p,\ldots,$ solve the lasso problem:
<span class='blue'>
$$ min_\beta \{ \frac{1}{2} \Vert W^{1/2}_{11} \beta - b\Vert^2 + \rho \Vert \beta \Vert_1 \}$$
</span>
   where $b = W^{-1/2}_{11} s_{12}$, which takes as input the inner products 
  $W_{11}$ and $s_{12}$.  This gives a $p -1$ vector solution $\hat{\beta}$. 
  Fill in the corresponding row and column $W$ using $w_{12} = W_{11} \hat{\beta}$.
3. Continue to convergence.

In `glasso`, the procedure stops when the average absolute change in $W$ is
less than $t \cdot  \text{ave} |S^{-\text{diag}}|$ where $S^{-\text{diag}}$ are
the off-diagonal elements of the empirical coveriance matrix $S$ and $t$ is
a fixed threshold, set by default at 0.001.

--- .segue .dark

## An interesting algorithm discussion...

---

## Performance

- Simulated data for sparse and dense scenarios:
- <span class='blue'>Sparse</span>
  *  $(\Sigma^{-1})_{ii} = 1$, 
  * $(\Sigma^{-1})_{i,i-1} = (\Sigma^{-1})_{i-1,i} = 0.5$
  * 0 otherwise
-. <span class='blue'>Dense</span>
  * $(\Sigma^{-1})_{ii} = 2$,
  * $(\Sigma^{-1})_{ii'} = 1$ otherwise

- Compared performance to `COVSEL` method from Banerjee et al. (2007).

- <span class='red'>Result</span>: Graphical lasso is 30-4000 times faster than
COVSEL and 2-10 slower than the approximate method.

- Even for dense problems, finishes in ~1min for p=1000 features. (Hard to tell
 from graph how it will scale to many more features, however).





---

## System info

```{r sysinfo}
sessionInfo()
```

<!-- Custom JavaScript -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script type='text/javascript'>
$(function() {
    $("p:has(img)").addClass('centered');
});
</script>


